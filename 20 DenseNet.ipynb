{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DenseNet\n",
    "因为 ResNet 提出了跨层链接的思想，这直接影响了随后出现的卷积网络架构，其中最有名的就是 cvpr 2017 的 best paper，DenseNet。\n",
    "\n",
    "DenseNet 和 ResNet 不同在于 ResNet 是跨层求和，而 DenseNet 是跨层将特征在通道维度进行拼接，下面可以看看他们两者的图示\n",
    "\n",
    "![](https://ws4.sinaimg.cn/large/006tNc79ly1fmpvj5vkfhj30uw0anq73.jpg)\n",
    "\n",
    "![](https://ws1.sinaimg.cn/large/006tNc79ly1fmpvj7fxd1j30vb0eyzqf.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第一张图是 ResNet，第二张图是 DenseNet，因为是在通道维度进行特征的拼接，所以底层的输出会保留进入所有后面的层，这能够更好的保证梯度的传播，同时能够使用低维的特征和高维的特征进行联合训练，能够得到更好的结果。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DenseNet 主要由 dense block 构成，下面我们来实现一个 densen block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "from torchvision.datasets import CIFAR10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先定义一个卷积块，这个卷积块的顺序是 bn -> relu -> conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_block(in_channel,out_channel):\n",
    "    layer = nn.Sequential(\n",
    "        nn.BatchNorm2d(in_channel),\n",
    "        nn.ReLU(True),\n",
    "        nn.Conv2d(in_channel,out_channel,3,padding=1,bias=False)\n",
    "    )\n",
    "    return layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dense block 将每次的卷积的输出称为 `growth_rate`，因为如果输入是 `in_channel`，有 n 层，那么输出就是 `in_channel + n * growh_rate`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dense_block(nn.Module):\n",
    "    def __init__(self,in_channel,growth_rate,num_layers):\n",
    "        super(dense_block,self).__init__()\n",
    "        block = []\n",
    "        channel = in_channel\n",
    "        for i in range(num_layers):\n",
    "            # .append() 方法用于在列表末尾添加新的对象\n",
    "            block.append(conv_block(channel,growth_rate))#输出growth_rate那么多的通道\n",
    "            channel += growth_rate #把两种通道加一起\n",
    "            \n",
    "        self.net = nn.Sequential(*block)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        for layer in self.net:\n",
    "            out = layer(x)\n",
    "            x = torch.cat((out,x),dim=1)#吧out和x的通道连接在一起，即实现了不同层特征的连接\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "验证一下输出的 channel 是否正确"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape: 3 x 96 x 96\n",
      "output shape: 39 x 96 x 96\n"
     ]
    }
   ],
   "source": [
    "test_net = dense_block(3, 12, 3)\n",
    "test_x = Variable(torch.zeros(1, 3, 96, 96))\n",
    "print('input shape: {} x {} x {}'.format(test_x.shape[1], test_x.shape[2], test_x.shape[3]))\n",
    "test_y = test_net(test_x)\n",
    "print('output shape: {} x {} x {}'.format(test_y.shape[1], test_y.shape[2], test_y.shape[3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "除了 dense block，DenseNet 中还有一个模块叫过渡层（transition block），因为 DenseNet 会不断地对维度进行拼接， 所以当层数很高的时候，输出的通道数就会越来越大，参数和计算量也会越来越大，为了避免这个问题，需要引入过渡层将输出通道降低下来，同时也将输入的长宽减半，这个过渡层可以使用 1 x 1 的卷积"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transition(in_channel, out_channel):\n",
    "    trans_layer = nn.Sequential(\n",
    "        nn.BatchNorm2d(in_channel),\n",
    "        nn.ReLU(True),\n",
    "        nn.Conv2d(in_channel, out_channel, 1),\n",
    "        nn.AvgPool2d(2, 2)\n",
    "    )\n",
    "    return trans_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "验证一下过渡层是否正确"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape: 3 x 96 x 96\n",
      "output shape: 12 x 48 x 48\n"
     ]
    }
   ],
   "source": [
    "test_net = transition(3, 12)\n",
    "test_x = Variable(torch.zeros(1, 3, 96, 96))\n",
    "print('input shape: {} x {} x {}'.format(test_x.shape[1], test_x.shape[2], test_x.shape[3]))\n",
    "test_y = test_net(test_x)\n",
    "print('output shape: {} x {} x {}'.format(test_y.shape[1], test_y.shape[2], test_y.shape[3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最后我们定义 DenseNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class densenet(nn.Module):\n",
    "    def __init__(self,in_channel,num_classes,growth_rate=32,block_layers=[6,12,24,16]):\n",
    "        super(densenet,self).__init__()\n",
    "        self.block1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channel,64,7,2,3),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(3,2,padding=1)\n",
    "        )\n",
    "        \n",
    "        channels = 64\n",
    "        block = []\n",
    "        for i,layers in enumerate(block_layers):\n",
    "            # .append() 方法用于在列表末尾添加新的对象\n",
    "            block.append(dense_block(channels,growth_rate,layers))\n",
    "            channels += layers * growth_rate\n",
    "            if i != len(block_layers) - 1:\n",
    "                block.append(transition(channels,channels // 2))#通过transition层将大小和通道数减半\n",
    "                channels = channels // 2\n",
    "        \n",
    "        self.block2 = nn.Sequential(*block)\n",
    "        self.block2.add_module('bn', nn.BatchNorm2d(channels))\n",
    "        self.block2.add_module('relu', nn.ReLU(True))\n",
    "        self.block2.add_module('avg_pool', nn.AvgPool2d(3))\n",
    "        \n",
    "        self.classifier = nn.Linear(channels, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.block1(x)\n",
    "        x = self.block2(x)\n",
    "        \n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output: torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "test_net = densenet(3, 10)#输入通道数3，分10类\n",
    "test_x = Variable(torch.zeros(1, 3, 96, 96))\n",
    "test_y = test_net(test_x)\n",
    "print('output: {}'.format(test_y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import train\n",
    "\n",
    "def data_tf(x):\n",
    "    x = x.resize((96, 96), 2) # 将图片放大到 96 x 96\n",
    "    x = np.array(x, dtype='float32') / 255\n",
    "    x = (x - 0.5) / 0.5 # 标准化，这个技巧之后会讲到\n",
    "    x = x.transpose((2, 0, 1)) # 将 channel 放到第一维，只是 pytorch 要求的输入方式\n",
    "    x = torch.from_numpy(x)\n",
    "    return x\n",
    "     \n",
    "train_set = CIFAR10('./data', train=True, transform=data_tf)\n",
    "train_data = torch.utils.data.DataLoader(train_set, batch_size=64, shuffle=True)\n",
    "test_set = CIFAR10('./data', train=False, transform=data_tf)\n",
    "test_data = torch.utils.data.DataLoader(test_set, batch_size=128, shuffle=False)\n",
    "\n",
    "net = densenet(3, 10)\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.01)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0. Train Loss: 1.383757, Train Acc: 0.503417, Valid Loss: 1.481526, Valid Acc: 0.481804, Time 00:02:03\n",
      "Epoch 1. Train Loss: 0.925259, Train Acc: 0.676850, Valid Loss: 1.658363, Valid Acc: 0.497132, Time 00:02:10\n",
      "Epoch 2. Train Loss: 0.706242, Train Acc: 0.756154, Valid Loss: 0.802267, Valid Acc: 0.723101, Time 00:02:09\n",
      "Epoch 3. Train Loss: 0.573607, Train Acc: 0.803269, Valid Loss: 0.814596, Valid Acc: 0.721618, Time 00:02:09\n",
      "Epoch 4. Train Loss: 0.478459, Train Acc: 0.836097, Valid Loss: 0.724878, Valid Acc: 0.756230, Time 00:02:09\n",
      "Epoch 5. Train Loss: 0.402810, Train Acc: 0.863671, Valid Loss: 0.859424, Valid Acc: 0.718948, Time 00:02:10\n",
      "Epoch 6. Train Loss: 0.336144, Train Acc: 0.885750, Valid Loss: 0.634214, Valid Acc: 0.785997, Time 00:02:12\n",
      "Epoch 7. Train Loss: 0.276543, Train Acc: 0.906710, Valid Loss: 0.581476, Valid Acc: 0.806962, Time 00:02:12\n",
      "Epoch 8. Train Loss: 0.223252, Train Acc: 0.925152, Valid Loss: 0.621072, Valid Acc: 0.800040, Time 00:02:12\n",
      "Epoch 9. Train Loss: 0.183673, Train Acc: 0.938239, Valid Loss: 0.733242, Valid Acc: 0.772646, Time 00:02:12\n",
      "Epoch 10. Train Loss: 0.149451, Train Acc: 0.950468, Valid Loss: 0.596565, Valid Acc: 0.819126, Time 00:02:12\n",
      "Epoch 11. Train Loss: 0.121576, Train Acc: 0.960238, Valid Loss: 0.946810, Valid Acc: 0.749110, Time 00:02:12\n",
      "Epoch 12. Train Loss: 0.102994, Train Acc: 0.966033, Valid Loss: 1.305171, Valid Acc: 0.699169, Time 00:02:12\n",
      "Epoch 13. Train Loss: 0.084743, Train Acc: 0.972566, Valid Loss: 0.736906, Valid Acc: 0.807358, Time 00:02:12\n",
      "Epoch 14. Train Loss: 0.061686, Train Acc: 0.980998, Valid Loss: 2.619342, Valid Acc: 0.556369, Time 00:02:13\n",
      "Epoch 15. Train Loss: 0.061593, Train Acc: 0.979999, Valid Loss: 0.619708, Valid Acc: 0.836036, Time 00:02:12\n",
      "Epoch 16. Train Loss: 0.044568, Train Acc: 0.986433, Valid Loss: 1.586963, Valid Acc: 0.685225, Time 00:02:13\n",
      "Epoch 17. Train Loss: 0.047498, Train Acc: 0.984974, Valid Loss: 0.623885, Valid Acc: 0.839597, Time 00:02:12\n",
      "Epoch 18. Train Loss: 0.033317, Train Acc: 0.990389, Valid Loss: 1.219098, Valid Acc: 0.737935, Time 00:02:13\n",
      "Epoch 19. Train Loss: 0.030206, Train Acc: 0.990929, Valid Loss: 0.731864, Valid Acc: 0.821301, Time 00:02:13\n"
     ]
    }
   ],
   "source": [
    "train(net, train_data, test_data, 20, optimizer, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "QL",
   "language": "python",
   "name": "ql"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
